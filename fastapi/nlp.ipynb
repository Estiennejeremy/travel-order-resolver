{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from enum import Enum\n",
    "from spacy.symbols import PROPN, NOUN, CCONJ, ADP, VERB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Création des dictionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direct(Enum):\n",
    "    NONE = 1\n",
    "    START = 2\n",
    "    DEST = 3\n",
    "\n",
    "class Force(Enum):\n",
    "    NONE = 1\n",
    "    WEAK = 2\n",
    "    STRONG = 3\n",
    "    \n",
    "class WordLinkSolo:\n",
    "    def __init__(self, word: str, direct: Direct, force: Force):\n",
    "        self.word = word\n",
    "        self.direct = direct\n",
    "        self.force = force\n",
    "\n",
    "class wordLink:\n",
    "    def __init__(self, word: str, fixedWord: str, direct: Direct, force: Force):\n",
    "        self.word = word\n",
    "        self.fixedWord = fixedWord\n",
    "        self.direct = direct\n",
    "        self.force = force\n",
    "\n",
    "\n",
    "\n",
    "LINK_NOUN_STRONG = [\n",
    "    WordLinkSolo(\"provenance\",     Direct.START, Force.STRONG),\n",
    "  \n",
    "]\n",
    "LINK_NOUN_WEAK = [\n",
    "    WordLinkSolo(\"direction\",      Direct.DEST,  Force.WEAK),\n",
    "    WordLinkSolo(\"destination\",    Direct.DEST,  Force.WEAK)\n",
    "]\n",
    "\n",
    "LINK_NOUN = LINK_NOUN_STRONG + LINK_NOUN_WEAK\n",
    "\n",
    "LINK_ADP_FIXED_START = [\n",
    "    wordLink(\"à\",\"partir\",       Direct.START, Force.STRONG),\n",
    "    wordLink(\"en\", \"partant\",    Direct.START, Force.STRONG),\n",
    "    wordLink(\"en\",\"passant\",   Direct.START,  Force.WEAK),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "LINK_ADP_FIXED_DEST = [\n",
    "    wordLink(\"à\",\"destination\",  Direct.DEST,  Force.STRONG),\n",
    "    wordLink(\"en\",\"direction\",   Direct.DEST,  Force.WEAK)\n",
    "]\n",
    "\n",
    "LINK_ADP_FIXED = LINK_ADP_FIXED_START + LINK_ADP_FIXED_DEST\n",
    "\n",
    "LINK_ADP_START = [\n",
    "    WordLinkSolo(\"de\",     Direct.START, Force.STRONG),\n",
    "    WordLinkSolo(\"du\",     Direct.START, Force.STRONG),\n",
    "    WordLinkSolo(\"des\",    Direct.START, Force.STRONG),\n",
    "    WordLinkSolo(\"depuis\", Direct.START, Force.STRONG),\n",
    "\n",
    "\n",
    "] \n",
    "\n",
    "LINK_ADP_DEST = [\n",
    "    WordLinkSolo(\"à\",      Direct.DEST,  Force.WEAK),\n",
    "    WordLinkSolo(\"dans\",   Direct.DEST,  Force.WEAK),\n",
    "    WordLinkSolo(\"par\",    Direct.DEST,  Force.WEAK) \n",
    "]\n",
    "\n",
    "LINK_ADP = LINK_ADP_DEST + LINK_ADP_START\n",
    "\n",
    "LINK_CCONJ_START = [\n",
    "    WordLinkSolo(\"depuis\",     Direct.START, Force.STRONG),\n",
    "]\n",
    "\n",
    "LINK_CCONJ_DEST = [\n",
    "    WordLinkSolo(\"puis\",       Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"et\",         Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"enfin\",      Direct.DEST,  Force.STRONG)\n",
    "]\n",
    "\n",
    "\n",
    "LINK_CCONJ = LINK_CCONJ_START + LINK_CCONJ_DEST\n",
    "\n",
    "LINK_VERB_MARK_START = [\n",
    "    WordLinkSolo(\"après\",   Direct.START, Force.WEAK),\n",
    "    WordLinkSolo(\"de\",   Direct.START, Force.STRONG),\n",
    "]\n",
    "\n",
    "LINK_VERB_MARK_DEST = [\n",
    "    WordLinkSolo(\"avant\",   Direct.DEST, Force.STRONG),\n",
    "]\n",
    "LINK_VERB_MARK = LINK_VERB_MARK_DEST + LINK_VERB_MARK_START\n",
    "\n",
    "LINK_VERB_START = [\n",
    "    WordLinkSolo(\"passer\",     Direct.START, Force.WEAK),\n",
    "    WordLinkSolo(\"être\",       Direct.START, Force.STRONG),\n",
    "   \n",
    "]\n",
    "\n",
    "LINK_VERB_DEST = [\n",
    "    WordLinkSolo(\"arriver\",    Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"aller\",      Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"visiter\",    Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"atterrir\",   Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"découvrir\",  Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"voyager\",    Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"rendre\",     Direct.DEST,  Force.STRONG)\n",
    "]\n",
    "\n",
    "LINK_VERB = LINK_VERB_START + LINK_VERB_DEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Logique algorithmique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: je voudrais aller à paris depuis Toulouse\n",
      "locs found: ['paris', 'Toulouse']\n",
      "paris\n",
      "Toulouse\n",
      "Token #1 : pari\n",
      "ADP: à type WEAK sens DEST\n",
      "VERB: aller type STRONG sens DEST\n",
      "Using: aller\n",
      "---------------\n",
      "Token #2 : Toulouse\n",
      "ADP: depuis type STRONG sens START\n",
      "VERB: aller type STRONG sens DEST\n",
      "Using: depuis\n",
      "---------------\n",
      "[Toulouse, paris]\n"
     ]
    }
   ],
   "source": [
    "### Paramètres:\n",
    "# i : position de la locations (associé au paramètre locs)\n",
    "# docs : objet résultant du traitement de la phrase par spacy\n",
    "# tokens : liste des tokens associés à des locations\n",
    "# locs : tableau des locations trouvées dans la phrase\n",
    "# tokenToped : boolean qui indique si un token associé à la destination a déjà été trouvée\n",
    "# advFor : type du mot (généré par spacy) à chercher (exemples -> VERB: verbe, DEST: destination)\n",
    "### Algorithme\n",
    "# Si aucun token associé à la destination n'a été trouvée alors\n",
    "# Pour tous les token générés par spacy\n",
    "# On regarde si il existe un token avec un type de mot égale à advFor\n",
    "# Si il est égale, on vérifie si il existe déjà dans la liste des tokens\n",
    "# Si il est déjà présent dans la liste des tokens on fait rien\n",
    "# Si le text du token est égale à la destination passé en paramètre on l'ajoute à la liste des tokens\n",
    "### Objectif\n",
    "# L'objectif est de vérifier si la location passé en paramètre correspond à un token généré par spacy\n",
    "# Et que le token trouvé possède un type de mot égale au paramètre advFor\n",
    "# Si l'on trouve un token on renvoie vrai\n",
    "def test_phrase(i, doc, tokens, locs, tokenToped, advFor ):\n",
    "    if tokenToped == False:\n",
    "        for token in doc:\n",
    "            if token is not None:\n",
    "                if advFor:\n",
    "                    if token.pos == advFor:\n",
    "                        isUsable = True\n",
    "                        for selectedTok in tokens:\n",
    "                            if type(selectedTok) != int and selectedTok == token:\n",
    "                                isUsable = False\n",
    "                        if isUsable:\n",
    "                            print(token)\n",
    "                            if token.text in locs[i]:\n",
    "                                tokens[i] = token\n",
    "                                return True\n",
    "\n",
    "### Paramètres:\n",
    "# docs : objet résultant du traitement de la phrase par spacy\n",
    "# tokens : liste des tokens associés à des locations\n",
    "# locs : tableau des locations trouvées dans la phrase\n",
    "# i : position de la locations associé au paramètre locs\n",
    "### Algorithme\n",
    "# Pour tous les token générés par spacy par rapport à la phrase\n",
    "# On regarde si le token existe dans la liste des tokens passés en paramètre\n",
    "# Si il est déjà présent dans la liste des tokens on fait rien\n",
    "# Si le text du token est égale à la destination passé en paramètre on l'ajoute à la liste des tokens\n",
    "### Objectif\n",
    "# L'objectif est de vérifier si la location passé en paramètre correspond à un token généré par spacy\n",
    "# Si l'on trouve un token on renvoie vrai\n",
    "def test_phrases_default(doc, tokens, locs, i ):\n",
    "     for token in doc:\n",
    "        isUsable = True\n",
    "        for tokenSelected in tokens:\n",
    "            if type(tokenSelected) != int and tokenSelected == token:\n",
    "                isUsable = False\n",
    "        if isUsable:\n",
    "            if token.text in locs[i]:\n",
    "                tokens[i] = token\n",
    "                return True\n",
    "\n",
    "                    \n",
    "### Paramètres:\n",
    "# elem : token généré par spacy\n",
    "# adp : type du mot (généré par spacy) à matcher (exemple -> VERB: verbe, DEST: destination)\n",
    "# rela : Dictionnaire de référence créer dans la partie 1 avec le poids et la direction associé au mot\n",
    "### Algorithme\n",
    "# Si le token possède un type de mot égale à adp\n",
    "# Pour chaque référence du dictionnaire\n",
    "# On regarde si la lématisation du token est égale à l'une des références du dictionnaire\n",
    "# Si il est égale, on arrete la boucle et on l'ajoute à fw (variable global)\n",
    "### Objectif\n",
    "# L'objectif est de vérifier si le token correspond à  l'une des références du dictionnaire (paramètre \"rela\")\n",
    "# si on trouve une référence alors on l'ajout au tableau fw (variable global)\n",
    "def finder(elem, adp, rela):\n",
    "    if elem.pos == adp:\n",
    "        for ref in rela:\n",
    "            if ref.word == elem.lemma_:\n",
    "                print(f\"CCONJ: {ref.word} with {ref.force.name} and {ref.direct.name}\")\n",
    "                fw.append(ref)\n",
    "                break\n",
    "                \n",
    "### Paramètres:\n",
    "# tokens : liste des tokens correspondants à une destination\n",
    "# position : position du token\n",
    "### Algorithme\n",
    "# Pour tous les tokens \"enfant\"\n",
    "# si le token enfant à pour type de text ADP alors\n",
    "# pour tous les sous enfants si la dépendance est égale à \"fixed\" alors\n",
    "# on parcours le dictionnaire de mot \"LINK_ADP_FIXED\"\n",
    "# si on trouve une référence du dictionnaire égale au texte lématisé du sous enfant \n",
    "# alors on l'ajout à fw (variable global)\n",
    "### Objectif\n",
    "# L'objectif est de vérifier parmis les tokens enfant (et ses sous enfant)\n",
    "# si l'un des token correspond à l'une des références du dictionnaire \"LINK_ADP_FIXED\"\n",
    "# si on trouve une référence alors on l'ajout au tableau fw (variable global)\n",
    "def LINK_ADP_FIXED_CHECK(tokens, i):\n",
    "    for child in tokens[i].children:\n",
    "        if child.pos == ADP:\n",
    "            for subChild in child.children:\n",
    "                if subChild.dep_ == 'fixed':\n",
    "                    for ref in LINK_ADP_FIXED:\n",
    "                        if ref.word == child.lemma_ and ref.fixedWord == subChild.lemma_:\n",
    "                            print(f\"ADP_FIXED: {ref.word} {ref.fixedWord} type {ref.force.name} sens {ref.direct.name}\")\n",
    "                            fw.append(ref)\n",
    "                            break\n",
    "\n",
    "### Paramètres:\n",
    "# tokens : liste des tokens correspondants à une destination\n",
    "# position : position du token\n",
    "### Algorithme\n",
    "# Pour tous les tokens \"enfant\"\n",
    "# on parcours le dictionnaire de mot \"LINK_ADP\"\n",
    "# si on trouve une référence du dictionnaire égale au texte lématisé du sous enfant \n",
    "# alors on l'ajout à fw (variable global)\n",
    "### Objectif\n",
    "# L'objectif est de vérifier parmis les tokens enfant\n",
    "# si l'un des token correspond à l'une des références du dictionnaire \"LINK_ADP\"\n",
    "# si on trouve une référence alors on l'ajout au tableau fw (variable global)\n",
    "def LINK_ADP_CHECK(tokens, i):\n",
    "    for child in tokens[i].children:\n",
    "        for ref in LINK_ADP:\n",
    "            if ref.word == child.lemma_:\n",
    "                print(f\"ADP: {ref.word} type {ref.force.name} sens {ref.direct.name}\")\n",
    "                fw.append(ref) \n",
    "\n",
    "### Paramètres:\n",
    "# parent : token généré par spacy\n",
    "### Algorithme\n",
    "# Si le token à pour type de text \"VERB\" (est un verbe) alors\n",
    "# on parcours tous ses enfants\n",
    "# si le token enfant à une dépendance de type \"mark\" et à pour type de text \"ADP\" alors\n",
    "# si on trouve une référence du dictionnaire \"LINK_VERB_MARK\" égale au texte lématisé du sous enfant \n",
    "# alors on l'ajout à fw (variable global)\n",
    "### Objectif\n",
    "# L'objectif est de vérifier parmis les tokens enfant\n",
    "# si l'un des tokens correspond à l'une des références du dictionnaire \"LINK_VERB_MARK\"\n",
    "# si on trouve une référence alors on l'ajout au tableau fw (variable global)\n",
    "def LINK_VERB_MARK_CHECK(parent):\n",
    "     if parent.pos == VERB:\n",
    "        for child in parent.children:\n",
    "            if child.dep_ == 'mark' and child.pos == ADP:\n",
    "                for ref in LINK_VERB_MARK:\n",
    "                    if ref.word == child.lemma_:\n",
    "                        print(f\" VERB: {ref.word} type {ref.force.name} sens {ref.direct.name}\")\n",
    "                        fw.append(ref)\n",
    "                        break\n",
    "\n",
    "### Paramètres:\n",
    "# parent : token généré par spacy\n",
    "### Algorithme\n",
    "# On parcours le dictionnaire de mot \"LINK_VERB\"\n",
    "# si on trouve une référence du dictionnaire égale au texte lématisé du token\n",
    "# alors on l'ajout à fw (variable global)\n",
    "### Objectif\n",
    "# L'objectif est de vérifier si le token correspond à l'une des références du dictionnaire \"LINK_VERB\"\n",
    "# si on trouve une référence alors on l'ajout au tableau fw (variable global)\n",
    "def LINK_VERB_CHECK(parent):\n",
    "    for ref in LINK_VERB:\n",
    "        if ref.word == parent.lemma_:\n",
    "            print(f\"VERB: {ref.word} type {ref.force.name} sens {ref.direct.name}\")\n",
    "            fw.append(ref)\n",
    "            break\n",
    "\n",
    "### Paramètres:\n",
    "# wToks : liste contenant le token et une référence d'un des dictionnaires (partie 1)\n",
    "### Algorithme\n",
    "# Pour tous les éléments dans wToks\n",
    "# si l'un des éléments possède une direction égale à START\n",
    "# après avoir modifier sa force, on ajoute le token à OrderedCities (variable global)\n",
    "# puis on appelle la fonction ORDER_DEST\n",
    "### Objectif\n",
    "# Cherche parmis wToks quel token correspond à une valeur de départ (avec une valeur de confiance)\n",
    "def ORDER_START(wToks):\n",
    "    sizeForce = 0\n",
    "    for i in range(len(wToks)):\n",
    "        token, weight = wToks[i]\n",
    "        if weight.direct == Direct.START:\n",
    "            if weight.force == Force.STRONG:\n",
    "                OrderedCities.insert(sizeForce, token)\n",
    "                sizeForce = sizeForce + 1\n",
    "            else:\n",
    "                OrderedCities.append(token)\n",
    "    ORDER_DEST(wToks)\n",
    "\n",
    "### Paramètres:\n",
    "# wToks : liste contenant le token et une référence d'un des dictionnaires (partie 1)\n",
    "### Algorithme\n",
    "# Pour tous les éléments dans wToks\n",
    "# si l'un des éléments possède une direction égale à DEST\n",
    "# après avoir modifier sa force, on ajoute le token à OrderedCities (variable global)\n",
    "### Objectif\n",
    "# Cherche parmis wToks quel token correspond à une valeur d'arrivée (avec une valeur de confiance)\n",
    "def ORDER_DEST(wToks):\n",
    "    sizeForce = 0\n",
    "    for i in range(len(wToks)):\n",
    "        token, weight = wToks[i]\n",
    "        if weight.direct == Direct.DEST:\n",
    "            if weight.force == Force.STRONG:\n",
    "                OrderedCities.append(token)\n",
    "                sizeForce = sizeForce + 1\n",
    "            else:\n",
    "                if sizeForce == 0:\n",
    "                    OrderedCities.append(token)\n",
    "                else:\n",
    "                    OrderedCities.insert(len(OrderedCities)-sizeForce, token)\n",
    "    \n",
    "### Paramètres:\n",
    "# sentence : phrase à analyser\n",
    "### Objectif\n",
    "# Trouver le point de départ et le point d'arrivée à partir d'une phrase\n",
    "def analyse(sentence):\n",
    "    \n",
    "    print(f\"Request: {sentence}\")\n",
    "    # Chargement du package francais \n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    # Traitement de la phrase par spacy -> tokenisation\n",
    "    doc = nlp(sentence)\n",
    "    # Initialisation du tableau contenant toute les locations\n",
    "    locs = []\n",
    "    \n",
    "    # On cherche parmis tous les mots ceux qui sont des locations ou des entité géopolitiques\n",
    "    # (c'est-à-dire pays, villes ou états) \n",
    "    for i in doc.ents:\n",
    "        if i.label_ == 'LOC' or i.label_ == 'GPE': \n",
    "            locs.append(i.text)\n",
    "    print(f\"locs found: {locs}\")\n",
    "\n",
    "    # Si on trouve aucune location on abandonne la requête sinon on continue\n",
    "    if len(locs) <= 1:\n",
    "        print(\"Cannot parse request or invalid request.\")\n",
    "    else:\n",
    "        global tokens\n",
    "        # On créer un tableau vide de même longueur que le tableau des locations trouvées\n",
    "        tokens = np.zeros(len(locs), dtype=object)\n",
    "        # Pour chaque élément du tableau \n",
    "        for i in range(len(locs)):\n",
    "            # Vrai si un token correspondant à la destionation a été trouvé, faux sinon\n",
    "            tokenToped = False\n",
    "            \n",
    "            # On vérifie si il existe un token avec un type de mot égale à PROPN correspond à la location\n",
    "            if test_phrase(i, doc, tokens, locs, tokenToped, PROPN):\n",
    "                tokenToped = True\n",
    "            \n",
    "            # On vérifie si il existe un token avec un type de mot égale à NOUN correspond à la location\n",
    "            if test_phrase(i, doc, tokens, locs, tokenToped, NOUN):\n",
    "                tokenToped = True\n",
    "            \n",
    "            # On vérifie si il existe un token correspond à la location\n",
    "            if tokenToped == False:\n",
    "                if test_phrases_default(doc, tokens, locs, i):\n",
    "                    tokenToped = True\n",
    "            \n",
    "            # Si aucun token correspondant à la location n'a été trouvé\n",
    "            # Alors on affiche un message d'erreur\n",
    "            # Et on ajoute une valeur vide\n",
    "            if tokenToped == False:\n",
    "                print(f\"Localization {locs[i]} not found\")\n",
    "                tokens[i] = None\n",
    "        \n",
    "        # On nettoie le tableau des tokens en suppriment l'entité/item égale à None\n",
    "        tmpTokens = tokens\n",
    "        tokens = [] \n",
    "        for token in tmpTokens: \n",
    "            if token != None : \n",
    "                tokens.append(token)\n",
    "\n",
    "        # On recréer un tableau vide de même longueur que le tableau des tokens liés au location\n",
    "        wToks = np.zeros(len(tokens), dtype=object)\n",
    "        # Pour chaque token\n",
    "        for i in range(len(tokens)):\n",
    "            # On affiche le text \"lématiser\" associé au token\n",
    "            # Exemples:\n",
    "            # \"nous\" -> \"nous\"\n",
    "            # \"sommes\" -> \"être\"\n",
    "            # \"forts\" -> \"fort\"\n",
    "            print(f\"Token #{i+1} : {tokens[i].lemma_}\")\n",
    "            global fw\n",
    "            fw = []\n",
    "            # On récupère le parent lié au mot\n",
    "            # Exemple:\n",
    "            # Pour la phrase \"J'aimerais aller à paris\"\n",
    "            # Le token de texte \"à\" aura pour parent le token de texte \"paris\"\n",
    "            parent = tokens[i].head\n",
    "            \n",
    "            # On récupère tous les mots associés au mot lié au token\n",
    "            # Exemples:\n",
    "            # Pour la phrase \"J'aimerais aller à paris\"\n",
    "            # Le token de texte \"paris\" aura pour enfant le token de texts \"à\"\n",
    "            # Le token de texte \"aller\" aura pour enfants les tokens de texts \"J'\", \"aimerais\", \"paris\"\n",
    "            for child in tokens[i].children:\n",
    "                finder(child,CCONJ,LINK_CCONJ)\n",
    "            \n",
    "            # Si aucun référence vers l'un des dictionnaires n'a été trouvé\n",
    "            # alors on regarde si le token parent correspond à une des rérérence du dictionnaire\n",
    "            if len(fw) <= 0: \n",
    "                finder(parent,NOUN,LINK_NOUN)\n",
    "\n",
    "            # Si aucune référence vers l'un des dictionnaires n'a été trouvé\n",
    "            # Alors on regarde parmis les enfants et les sous enfants si\n",
    "            # l'un des tokens correspons à une référence du dictionnaire \"LINK_ADP_FIXED\"\n",
    "            if len(fw) <= 0: \n",
    "                LINK_ADP_FIXED_CHECK(tokens, i)\n",
    "\n",
    "                \n",
    "            # Si aucune référence vers l'un des dictionnaires n'a été trouvé\n",
    "            # Alors on regarde parmis les enfants si\n",
    "            # l'un des tokens correspons à une référence du dictionnaire \"LINK_ADP\"        \n",
    "            if len(fw) <= 0:\n",
    "                LINK_ADP_CHECK(tokens, i)\n",
    "            \n",
    "            # Si aucune référence vers l'un des dictionnaires n'a été trouvé\n",
    "            # Alors on regarde parmis les enfants si\n",
    "            # l'un des tokens correspons à une référence du dictionnaire \"LINK_VERB_MARK\"    \n",
    "            if len(fw) <= 1:\n",
    "                LINK_VERB_MARK_CHECK(parent)\n",
    "            \n",
    "            # Si aucune référence vers l'un des dictionnaires n'a été trouvé\n",
    "            # Alors on regarde si le token correspons à une référence du dictionnaire \"LINK_VERB\"   \n",
    "            if len(fw) <= 1:\n",
    "                LINK_VERB_CHECK(parent)\n",
    "            \n",
    "            # Si aucune référence vers l'un des dictionnaires n'a été trouvé\n",
    "            # Alors on ajoute une valeur par défaut à fw\n",
    "            if len(fw) == 0: \n",
    "                print(f\"Using default weight\")\n",
    "                fw.append(WordLinkSolo(\"default\", Direct.DEST,  Force.WEAK))\n",
    "            \n",
    "            \n",
    "            # Parmis toutes les références trouvées\n",
    "            # si on trouve une référence avec un force forte alors on la copie dans seletedWeight\n",
    "            # sinon on ajout la première valeur de fw\n",
    "            selectedWeight = None\n",
    "            for j in range(len(fw)):\n",
    "                if fw[j].force == Force.STRONG:\n",
    "                    selectedWeight = fw[j]\n",
    "                    break\n",
    "            if selectedWeight is None:\n",
    "                selectedWeight = fw[0]\n",
    "            \n",
    "            # On assigne la valeur selectedWeight avec le token dans wToks\n",
    "            print(f\"Using: {selectedWeight.word}\")\n",
    "            print(\"---------------\")\n",
    "            wToks[i] = (tokens[i], selectedWeight)\n",
    "        \n",
    "        # On cherche parmis les résultats quel token correspond à un point de départ\n",
    "        # et quel token token correspond à un point d'arriver avec une valeur de confiance\n",
    "        global OrderedCities\n",
    "        OrderedCities = []\n",
    "        ORDER_START(wToks)\n",
    "        return OrderedCities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(analyse(\"je voudrais aller à paris depuis Toulouse\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
