{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from enum import Enum\n",
    "from spacy.symbols import PROPN, NOUN, CCONJ, ADP, VERB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direct(Enum):\n",
    "    NONE = 1\n",
    "    START = 2\n",
    "    DEST = 3\n",
    "\n",
    "class Force(Enum):\n",
    "    NONE = 1\n",
    "    WEAK = 2\n",
    "    STRONG = 3\n",
    "    \n",
    "class WordLinkSolo:\n",
    "    def __init__(self, word: str, direct: Direct, force: Force):\n",
    "        self.word = word\n",
    "        self.direct = direct\n",
    "        self.force = force\n",
    "\n",
    "class wordLink:\n",
    "    def __init__(self, word: str, fixedWord: str, direct: Direct, force: Force):\n",
    "        self.word = word\n",
    "        self.fixedWord = fixedWord\n",
    "        self.direct = direct\n",
    "        self.force = force\n",
    "\n",
    "\n",
    "\n",
    "LINK_NOUN_STRONG = [\n",
    "    WordLinkSolo(\"provenance\",     Direct.START, Force.STRONG),\n",
    "  \n",
    "]\n",
    "LINK_NOUN_WEAK = [\n",
    "    WordLinkSolo(\"direction\",      Direct.DEST,  Force.WEAK),\n",
    "    WordLinkSolo(\"destination\",    Direct.DEST,  Force.WEAK)\n",
    "]\n",
    "\n",
    "LINK_NOUN = LINK_NOUN_STRONG + LINK_NOUN_WEAK\n",
    "\n",
    "LINK_ADP_FIXED_START = [\n",
    "    wordLink(\"à\",\"partir\",       Direct.START, Force.STRONG),\n",
    "    wordLink(\"en\", \"partant\",    Direct.START, Force.STRONG),\n",
    "    wordLink(\"en\",\"passant\",   Direct.START,  Force.WEAK),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "LINK_ADP_FIXED_DEST = [\n",
    "    wordLink(\"à\",\"destination\",  Direct.DEST,  Force.STRONG),\n",
    "    wordLink(\"en\",\"direction\",   Direct.DEST,  Force.WEAK)\n",
    "]\n",
    "\n",
    "LINK_ADP_FIXED = LINK_ADP_FIXED_START + LINK_ADP_FIXED_DEST\n",
    "\n",
    "LINK_ADP_START = [\n",
    "    WordLinkSolo(\"de\",     Direct.START, Force.STRONG),\n",
    "    WordLinkSolo(\"du\",     Direct.START, Force.STRONG),\n",
    "    WordLinkSolo(\"des\",    Direct.START, Force.STRONG),\n",
    "    WordLinkSolo(\"depuis\", Direct.START, Force.STRONG),\n",
    "\n",
    "\n",
    "] \n",
    "\n",
    "LINK_ADP_DEST = [\n",
    "    WordLinkSolo(\"à\",      Direct.DEST,  Force.WEAK),\n",
    "    WordLinkSolo(\"dans\",   Direct.DEST,  Force.WEAK),\n",
    "    WordLinkSolo(\"par\",    Direct.DEST,  Force.WEAK) \n",
    "]\n",
    "\n",
    "LINK_ADP = LINK_ADP_DEST + LINK_ADP_START\n",
    "\n",
    "LINK_CCONJ_START = [\n",
    "    WordLinkSolo(\"depuis\",     Direct.START, Force.STRONG),\n",
    "]\n",
    "\n",
    "LINK_CCONJ_DEST = [\n",
    "    WordLinkSolo(\"puis\",       Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"et\",         Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"enfin\",      Direct.DEST,  Force.STRONG)\n",
    "]\n",
    "\n",
    "\n",
    "LINK_CCONJ = LINK_CCONJ_START + LINK_CCONJ_DEST\n",
    "\n",
    "LINK_VERB_MARK_START = [\n",
    "    WordLinkSolo(\"après\",   Direct.START, Force.WEAK),\n",
    "    WordLinkSolo(\"de\",   Direct.START, Force.STRONG),\n",
    "]\n",
    "\n",
    "LINK_VERB_MARK_DEST = [\n",
    "    WordLinkSolo(\"avant\",   Direct.DEST, Force.STRONG),\n",
    "]\n",
    "LINK_VERB_MARK = LINK_VERB_MARK_DEST + LINK_VERB_MARK_START\n",
    "\n",
    "LINK_VERB_START = [\n",
    "    WordLinkSolo(\"passer\",     Direct.START, Force.WEAK),\n",
    "    WordLinkSolo(\"être\",       Direct.START, Force.STRONG),\n",
    "   \n",
    "]\n",
    "\n",
    "LINK_VERB_DEST = [\n",
    "    WordLinkSolo(\"arriver\",    Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"aller\",      Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"visiter\",    Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"atterrir\",   Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"découvrir\",  Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"voyager\",    Direct.DEST,  Force.STRONG),\n",
    "    WordLinkSolo(\"rendre\",     Direct.DEST,  Force.STRONG)\n",
    "]\n",
    "\n",
    "LINK_VERB = LINK_VERB_START + LINK_VERB_DEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: je voudrais aller à paris depuis Toulouse\n",
      "locs found: ['paris', 'Toulouse']\n",
      "paris\n",
      "Toulouse\n",
      "Token #1 : pari\n",
      "ADP: à with WEAK and DEST\n",
      "VERB: aller with STRONG and DEST\n",
      "Using: aller\n",
      "---------------\n",
      "Token #2 : Toulouse\n",
      "ADP: depuis with STRONG and START\n",
      "VERB: aller with STRONG and DEST\n",
      "Using: depuis\n",
      "---------------\n",
      "[Toulouse, paris]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_phrase(i, doc, tokens, locs, tokenToped, advFor ):\n",
    "    if tokenToped == False:\n",
    "        for token in doc:\n",
    "            if token is not None:\n",
    "                if advFor:\n",
    "                    if token.pos == advFor:\n",
    "                        isUsable = True\n",
    "                        for selectedTok in tokens:\n",
    "                            if type(selectedTok) != int and selectedTok == token:\n",
    "                                isUsable = False\n",
    "                        if isUsable:\n",
    "                            print(token)\n",
    "                            if token.text in locs[i]:\n",
    "                                tokens[i] = token\n",
    "                                return True\n",
    "                            \n",
    "def test_phrases_default(doc, tokens, locs, i ):\n",
    "     for token in doc:\n",
    "        isUsable = True\n",
    "        for tokenSelected in tokens:\n",
    "            if type(tokenSelected) != int and tokenSelected == token:\n",
    "                isUsable = False\n",
    "        if isUsable:\n",
    "            if token.text in locs[i]:\n",
    "                tokens[i] = token\n",
    "                return True\n",
    "\n",
    "                    \n",
    "                            \n",
    "def finder(elem, adp, rela):\n",
    "    if elem.pos == adp:\n",
    "        for ref in rela:\n",
    "            if ref.word == elem.lemma_:\n",
    "                print(f\"CCONJ: {ref.word} with {ref.force.name} and {ref.direct.name}\")\n",
    "                fw.append(ref)\n",
    "                break\n",
    "\n",
    "def LINK_ADP_FIXED_CHECK(tokens, i):\n",
    "    for child in tokens[i].children:\n",
    "        if child.pos == ADP:\n",
    "            for subChild in child.children:\n",
    "                if subChild.dep_ == 'fixed':\n",
    "                    for ref in LINK_ADP_FIXED:\n",
    "                        if ref.word == child.lemma_ and ref.fixedWord == subChild.lemma_:\n",
    "                            print(f\"ADP_FIXED: {ref.word} {ref.fixedWord} type {ref.force.name} sens {ref.direct.name}\")\n",
    "                            fw.append(ref)\n",
    "                            break\n",
    "def LINK_ADP_CHECK(tokens, i):\n",
    "    for child in tokens[i].children:\n",
    "        for ref in LINK_ADP:\n",
    "            if ref.word == child.lemma_:\n",
    "                print(f\"ADP: {ref.word} type {ref.force.name} sens {ref.direct.name}\")\n",
    "                fw.append(ref) \n",
    "                \n",
    "def LINK_VERB_MARK_CHECK(parent):\n",
    "     if parent.pos == VERB:\n",
    "        for child in parent.children:\n",
    "            if child.dep_ == 'mark' and child.pos == ADP:\n",
    "                for ref in LINK_VERB_MARK:\n",
    "                    if ref.word == child.lemma_:\n",
    "                        print(f\" VERB: {ref.word} type {ref.force.name} sens {ref.direct.name}\")\n",
    "                        fw.append(ref)\n",
    "                        break\n",
    "                        \n",
    "def LINK_VERB_CHECK(parent):\n",
    "    for ref in LINK_VERB:\n",
    "        if ref.word == parent.lemma_:\n",
    "            print(f\"VERB: {ref.word} type {ref.force.name} sens {ref.direct.name}\")\n",
    "            fw.append(ref)\n",
    "            break\n",
    "            \n",
    "def ORDER_START(wToks):\n",
    "    sizeForce = 0\n",
    "    for i in range(len(wToks)):\n",
    "        token, weight = wToks[i]\n",
    "        if weight.direct == Direct.START:\n",
    "            if weight.force == Force.STRONG:\n",
    "                OrderedCities.insert(sizeForce, token)\n",
    "                sizeForce = sizeForce + 1\n",
    "            else:\n",
    "                OrderedCities.append(token)\n",
    "    ORDER_DEST(wToks)\n",
    "\n",
    "def ORDER_DEST(wToks):\n",
    "    sizeForce = 0\n",
    "    for i in range(len(wToks)):\n",
    "        token, weight = wToks[i]\n",
    "        if weight.direct == Direct.DEST:\n",
    "            if weight.force == Force.STRONG:\n",
    "                OrderedCities.append(token)\n",
    "                sizeForce = sizeForce + 1\n",
    "            else:\n",
    "                if sizeForce == 0:\n",
    "                    OrderedCities.append(token)\n",
    "                else:\n",
    "                    OrderedCities.insert(len(OrderedCities)-sizeForce, token)\n",
    "    \n",
    "\n",
    "def analyse(sentence, nlp):\n",
    "    print(f\"Request: {sentence}\")\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    locs = []\n",
    "    fullTrip = []\n",
    "    for i in doc.ents:\n",
    "        if i.label_ == 'LOC' or i.label_ == 'GPE': \n",
    "            locs.append(i.text)\n",
    "    print(f\"locs found: {locs}\")\n",
    "\n",
    "    if len(locs) <= 1:\n",
    "        print(\"Cannot parse request or invalid request.\")\n",
    "    else:\n",
    "        global tokens\n",
    "        tokens = np.zeros(len(locs), dtype=object)\n",
    "        for i in range(len(locs)):\n",
    "           \n",
    "            tokenToped = False\n",
    "\n",
    "            if test_phrase(i, doc, tokens, locs, tokenToped, PROPN):\n",
    "                tokenToped = True\n",
    "\n",
    "            if test_phrase(i, doc, tokens, locs, tokenToped, NOUN):\n",
    "                tokenToped = True\n",
    "                \n",
    "            if tokenToped == False:\n",
    "                if test_phrases_default(doc, tokens, locs, i):\n",
    "                    tokenToped = True\n",
    "            \n",
    "            if tokenToped == False:\n",
    "                print(f\"Localization {locs[i]} not found\")\n",
    "                tokens[i] = None\n",
    "\n",
    "        tmpTokens = tokens\n",
    "        tokens = [] \n",
    "        for token in tmpTokens: \n",
    "            if token != None : \n",
    "                tokens.append(token)\n",
    "\n",
    "\n",
    "        wToks = np.zeros(len(tokens), dtype=object)\n",
    "        for i in range(len(tokens)):\n",
    "            print(f\"Token #{i+1} : {tokens[i].lemma_}\")\n",
    "            global fw\n",
    "            fw = []\n",
    "            parent = tokens[i].head\n",
    "\n",
    "            for child in tokens[i].children:\n",
    "                finder(child,CCONJ,LINK_CCONJ)\n",
    "\n",
    "            if len(fw) <= 0: \n",
    "                finder(parent,NOUN,LINK_NOUN)\n",
    "\n",
    "\n",
    "            if len(fw) <= 0: \n",
    "                LINK_ADP_FIXED_CHECK(tokens, i)\n",
    "\n",
    "                \n",
    "                    \n",
    "            if len(fw) <= 0:\n",
    "                LINK_ADP_CHECK(tokens, i)\n",
    "\n",
    "            if len(fw) <= 1:\n",
    "                LINK_VERB_MARK_CHECK(parent)\n",
    "                \n",
    "            if len(fw) <= 1:\n",
    "                LINK_VERB_CHECK(parent)\n",
    "                \n",
    "            if len(fw) == 0: \n",
    "                print(f\"Using default weight\")\n",
    "                fw.append(WordLinkSolo(\"default\", Direct.DEST,  Force.WEAK))\n",
    "\n",
    "            \n",
    "            selectedWeight = None\n",
    "            for j in range(len(fw)):\n",
    "                if fw[j].force == Force.STRONG:\n",
    "                    selectedWeight = fw[j]\n",
    "                    break\n",
    "            if selectedWeight is None:\n",
    "                selectedWeight = fw[0]\n",
    "\n",
    "            print(f\"Using: {selectedWeight.word}\")\n",
    "            print(\"---------------\")\n",
    "            wToks[i] = (tokens[i], selectedWeight)\n",
    "\n",
    "        global OrderedCities\n",
    "        OrderedCities = []\n",
    "        ORDER_START(wToks)\n",
    "        return OrderedCities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(analyse(\"je voudrais aller à paris depuis Toulouse\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
